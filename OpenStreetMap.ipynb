{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Data Wrangling Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenStreetMap Sample Project - Data Wrangling with MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Data Preparation\n",
    "\n",
    "**Map Area: Saint-Joseph - Île de La Réunion**\n",
    "- http://www.openstreetmap.org/relation/1282272#map=12/-21.2918/55.6440\n",
    "- http://overpass-api.de/api/map?bbox=55.4871,-21.4039,55.8009,-21.1796"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='LaReunion.png', width=300, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import OpenStreetMapTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OSM_FILE = \"Saint-Joseph.La-Reunion.osm\" \n",
    "SAMPLE_FILE = \"Saint-Joseph.La-Reunion.sample.osm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 - Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OpenStreetMapTools.process_sample(file_in=OSM_FILE, file_out=SAMPLE_FILE, k=K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 - node and way xml tags\n",
    "\n",
    "**Data Source:** https://www.data.gouv.fr/s/resources/fichier-fantoir-des-voies-et-lieux-dits/community/20150512-103719/Descriptif_FANTOIR.pdf\n",
    "\n",
    "From FANTOIR database we extract all possible well spelled Way Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FANTOIR1016_WAY_TYPE = pd.read_csv(\"FANTOIR2016-WAY-TYPE.csv\", sep=\";\")\n",
    "expected_way_types = FANTOIR1016_WAY_TYPE.LIBELLE.apply(lambda x: x.title()).values\n",
    "FANTOIR1016_WAY_TYPE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st_types = OpenStreetMapTools.audit(SAMPLE_FILE, expected_way_types)\n",
    "pprint.pprint(dict(st_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the sample data above, we can identify at least the following problems:\n",
    "1. Street types are wrongling spelled or abbreviated\n",
    "2. No street types\n",
    "3. Mixed encoding type\n",
    "4. Street name case not consistent (Full upercase or lowercase or mix)\n",
    "\n",
    "##### A first cleansing approach to address the problems listed above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Map to the correct Street type \n",
    "street_type_mapping = { \n",
    "    \"ALLEE\": \"Allee\",\n",
    "    \"Allee\": \"Allee\",\n",
    "    \"Alle\": \"Allee\",\n",
    "    u'All\\xe8e': \"Allee\",\n",
    "    u'All\\xe9e': \"Allee\",\n",
    "    \"avenue\": \"Avenue\",\n",
    "    \"Chemain\": \"Chemin\",\n",
    "    \"Ch.\": \"Chemin\",\n",
    "    \"Ch \": \"Chemin\",\n",
    "    \"Ch\": \"Chemin\",\n",
    "    \"Che\": \"Chemin\",\n",
    "    \"Cemin\": \"Chemin\",\n",
    "    \"chemin\": \"Chemin\",\n",
    "    \"Ch.Lolotte\": \"Chemin Lolotte\",\n",
    "    \"Imp\": \"Impasse\",\n",
    "    \"Imp.\": \"Impasse\",\n",
    "    \"IMPASSE\": \"Impasse\",\n",
    "    \"impasse\": \"Impasse\",\n",
    "    \"IMPdes\": \"Impasse des\",\n",
    "    \"RUE\": \"Rue\",\n",
    "    \"Voi\": \"Voie\"\n",
    "}\n",
    "\n",
    "# Update wrongly spelled or abbreviated Street names \n",
    "# and complete if necesseray (i.e.: \"\"Katia et Maurice Kraft\": \"Rue Katia et Maurice Kraft\")\n",
    "street_name_mapping = {\n",
    "    \"Jean Lauret\": \"Lieu-dit Jean Lauret\",\n",
    "    \"Fortune Hoarau\": u\"Rue Fortuné Hoarau\",\n",
    "    \"Sentier Prunes\": \"Sentier Des Prunes\",\n",
    "    u\"allée des flamant\": \"Allee Des Flamants\",\n",
    "    \"Che De L'Ariege\": \"Chemin De L'Ariege\",\n",
    "    \"Katia et Maurice Kraft\": \"Rue Katia et Maurice Kraft\",\n",
    "    \"Grande Savanne\": \"Lieu-dit Grande Savanne\",\n",
    "    \"Hubert Delisle\": \"Rue Hubert Delisle\",\n",
    "    \"Ch Des Gueri Vit\": \"Impasse Guerit Vite\",\n",
    "    \"Voi cite les saphirs\": \"Voie Cite les saphirs\"\n",
    "}\n",
    "\n",
    "# Choice is made to avoid latin characters at this level of cleansing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for st_type, ways in st_types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = OpenStreetMapTools.update_name(name, street_type_mapping, street_name_mapping)\n",
    "        print name, \"=>\", better_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 - Full Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 - node and way xml tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_st_types = OpenStreetMapTools.audit(OSM_FILE, expected_way_types)\n",
    "pprint.pprint(dict(full_st_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for full_st_types, ways in st_types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = OpenStreetMapTools.update_name(name, street_type_mapping, street_name_mapping)\n",
    "        print name, \"=>\", better_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleansing developed on the sample dataset seems to address the main issues present in the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - From XML to JSON to MongoDB\n",
    "\n",
    "#### 2.1 - Create JSON file\n",
    "\n",
    "Based on the cleansing identified in previous section, Street names will be partially updated during the JSON file generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ways_json_data = OpenStreetMapTools.process_map(OSM_FILE, street_type_mapping, street_name_mapping, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[x for x in ways_json_data if x[\"id\"] == \"3480487005\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 - MongoDB Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mongo OpenStreetMap --eval \"db.dropDatabase()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mongoimport -d OpenStreetMap -c LaReunion --file data/Saint-Joseph.La-Reunion.osm.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 - Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from bson.son import SON\n",
    "\n",
    "client = MongoClient()\n",
    "\n",
    "def pretty(documents):\n",
    "    for document in documents:\n",
    "        pprint.pprint(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = client.OpenStreetMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just peek the previously selected sample example from our python cleansing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pretty(db.LaReunion.find({\"id\": \"3480487005\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we have the same count as the result of our python cleansing code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.LaReunion.count() == len(ways_json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of \"xml node\" imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    {\"$unwind\": \"$type\"},\n",
    "    {\"$group\": {\"_id\": \"$type\", \"count\": {\"$sum\": 1}}},\n",
    "    {\"$sort\": SON([(\"count\", -1), (\"_id\", -1)])}\n",
    "]\n",
    "nodes = pd.DataFrame(list(db.LaReunion.aggregate(pipeline)))\n",
    "nodes.columns = [\"node\", \"count\"]\n",
    "nodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many different users have contributed to this database?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(db.LaReunion.find().distinct(\"created.user\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When did the contribution take place?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    {\"$project\": {\"_id\": False, \"timestamp\":  \"$created.timestamp\", \"user\": \"$created.user\" } },\n",
    "    {\"$group\": {\"_id\": \"$timestamp\",  \"users\": { \"$sum\": 1 }}}\n",
    "]\n",
    "contributions = pd.DataFrame(list(db.LaReunion.aggregate(pipeline)))\n",
    "contributions[\"_id\"] = pd.to_datetime(contributions._id)\n",
    "contributions.columns = [\"date\", \"users\"]\n",
    "axes = contributions.set_index(['date']).plot(figsize=(12,6), title=\"Number of users contribution by date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have quite recent contributions for this dataset, let's identify the top 10 contributors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    { \"$project\": { \"_id\": False, \"user\": \"$created.user\" } },\n",
    "    { \"$group\": { \"_id\": \"$user\", \"count\": { \"$sum\": 1 } } },\n",
    "    { \"$sort\": SON([(\"count\", -1), (\"_id\", -1)]) },\n",
    "    { \"$limit\": 10 }\n",
    "]\n",
    "pretty(list(db.LaReunion.aggregate(pipeline)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider having 1 top contributor followed by 5 high contributors... let see the distribution of the contribution for all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    { \"$project\": { \"_id\": False, \"user\": \"$created.user\" } },\n",
    "    { \"$group\": { \"_id\": \"$user\", \"count\": { \"$sum\": 1 } } },\n",
    "    { \"$sort\": SON([(\"count\", -1), (\"_id\", -1)]) },\n",
    "    { \"$project\": { \"_id\": \"$count\"} }\n",
    "]\n",
    "contributions = pd.DataFrame(list(db.LaReunion.aggregate(pipeline)))\n",
    "contributions.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above ouput clearly show that we have outliers in term of user contribution... just see if mongodb can provide us the distribution of the all users contribution with a predefined bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    { \"$project\": { \"_id\": False, \"user\": \"$created.user\" } },\n",
    "    { \"$group\": { \"_id\": \"$user\", \"count\": { \"$sum\": 1 } } },\n",
    "    {\n",
    "        \"$bucket\": {\n",
    "            \"groupBy\": \"$count\",\n",
    "            \"boundaries\": [ 1, 100, 10000, 25000, 100000 ],\n",
    "            \"default\": \"TOP\",\n",
    "            \"output\": {\n",
    "              \"count\": { \"$sum\": 1 },\n",
    "              \"users\": { \"$push\": \"$_id\" }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "contributions = pd.DataFrame(list(db.LaReunion.aggregate(pipeline)))\n",
    "contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result confirm that most user are below 100 contributions and the main contributors are composed of 5 high contributors  and TOP contributor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - How MongoDb will ease the data cleansing process...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First focus on all document with type='node' and having a subdocument 'address'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    { \"$match\": { \"type\": \"node\", \"address\": { \"$exists\": True } } },\n",
    "    { \"$project\": { \"_id\": False, \n",
    "                   \"city\": \"$address.city\", \n",
    "                   \"housenumber\": \"$address.housenumber\", \n",
    "                   \"postcode\": \"$address.postcode\", \n",
    "                   \"street\": \"$address.street\" } },\n",
    "    #{ \"$limit\": 10 }\n",
    "]\n",
    "addresses = pd.DataFrame(list(db.LaReunion.aggregate(pipeline)))\n",
    "addresses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addresses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some missing data for all selected attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addresses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 - City\n",
    "\n",
    "Let see if we can identify any problems with city attribute...  Can we recover the missing data for the city? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be possible if the postcode is not null... to crosscheck with the postcal code as available at \"http://datanova.legroupe.laposte.fr/explore/dataset/laposte_hexasmal/download/?format=csv&timezone=Europe/Berlin&use_labels_for_header=true\"\n",
    "\n",
    "Content is described at \"https://www.data.gouv.fr/fr/datasets/base-officielle-des-codes-postaux/\", we will the following fields Nom_commune\tand Code_postal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "url=\"http://datanova.legroupe.laposte.fr/explore/dataset/laposte_hexasmal/download/?format=csv&timezone=Europe/Berlin&use_labels_for_header=true\"\n",
    "postalcodes=pd.read_csv(io.StringIO(requests.get(url).content.decode('utf-8')), sep=\";\")\n",
    "postalcodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "codes = dict(zip(postalcodes.Code_postal, postalcodes.Nom_commune))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "citiesFromPostCode = addresses[(addresses.city.isnull() == True) & (addresses.postcode.isnull() == False)]\n",
    "citiesFromPostCode.loc[:, (\"city\")] = citiesFromPostCode.postcode.apply(lambda x: codes[int(x)].title())\n",
    "citiesFromPostCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addresses.city.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two additional name to be clean \"u'Petite-\\xcele'\" and \"u'Petit \\xeele'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a big deal... cities in our dataset with some capitalization issues, wrong spelling and one dash missing in name, we should have the following cities spelled as follows: __Le Tampon, Saint-Pierre, Saint-Joseph, Petite-Ile...__. 'nan' value could not be updated\n",
    "\n",
    "We can quickly solve this with python calling title() and replacing space with a dash for some cities...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanName(x):\n",
    "    if x == u'Petite-\\xcele' or x == u'Petit \\xeele':\n",
    "        return u'Petite-Ile'\n",
    "    elif x.startswith(\"Saint\") or x.startswith(\"Petite\"):\n",
    "        return x.replace(\" \", \"-\").title()\n",
    "    elif x == \"Petit\" or x.startswith(\"Petit \") or x.startswith(\"PETIT\"):\n",
    "        return \"Petite-Ile\"\n",
    "    elif x in [\"Le Tampion\", \"Tampon\"]:\n",
    "        return \"Le Tampon\"\n",
    "    elif x.startswith(\"St\"):\n",
    "        return x.replace(\"St\", \"Saint\").title()\n",
    "    else:\n",
    "        return x.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addresses.loc[:, (\"city\")] = addresses.city.dropna().apply(lambda x: str(cleanName(x)))\n",
    "addresses.city.dropna().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rev_codes = {v: k for k, v in codes.items()}\n",
    "def reviewCityNameForLookup(x):\n",
    "    return rev_codes[x.upper().replace(\"-\", \" \").replace(\"SAINT\", \"ST\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "citiesToPostCode = addresses[(addresses.city.isnull() == False) & (addresses.postcode.isnull() == True)]\n",
    "\n",
    "citiesToPostCode.loc[:, (\"postcode\")] = citiesToPostCode.city.dropna().apply(lambda x: reviewCityNameForLookup(x))\n",
    "citiesToPostCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(addresses.postcode.dropna().unique()) == len(addresses.city.dropna().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code addressed some issues identified in the City and PostCode fields, what about street? What are the issues?\n",
    "\n",
    "We are going to use the same FANTOIR DATABASE as referenced above.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FANTOIR1016 = pd.read_table(\"FANTOIR1016\", header=None)\n",
    "FANTOIR1016 = FANTOIR1016[FANTOIR1016[0].str.startswith(\"974\") == True]\n",
    "FANTOIR1016.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we are only considering the first 41 characters in FANTOIR1016 database (see §3.4 of referenced document)\n",
    "\n",
    "| Code département | Code direction | Code commune | Identifiant de la voie dans la commune | Clé RIVOLI | Code nature de voie | Libellé voie |\n",
    "\n",
    "Next we are filtering all lines not having at least 11 characters before a first space\n",
    "Last steps: split the data into two groupes\n",
    "- | Code département | Code direction | Code commune | Identifiant de la voie dans la commune | Clé RIVOLI\n",
    "- | Code nature de voie | Libellé voie |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FANTOIR1016_974 = FANTOIR1016[0].apply(lambda x : pd.Series(x[:41]))\n",
    "FANTOIR1016_974[\"KEEP\"] = FANTOIR1016_974[0].apply(lambda x: len(x.split(' ')[0]) >= 11)\n",
    "FANTOIR1016_974 = FANTOIR1016_974[FANTOIR1016_974.KEEP == True][0].apply(lambda x: pd.Series([x[:11].strip(), \n",
    "                                                                                              x[11:15].strip(), \n",
    "                                                                                              x[15:41].strip()]))\n",
    "FANTOIR1016_974.columns = [\"REFERENCE\", \"CODE\", \"VOIE\"]\n",
    "FANTOIR1016_974.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FANTOIR1016_974_WAYS = pd.merge(left=FANTOIR1016_974, right=FANTOIR1016_WAY_TYPE, on=\"CODE\")[[\"REFERENCE\",\n",
    "                                                                                              \"CODE\", \n",
    "                                                                                              \"LIBELLE\", \n",
    "                                                                                              \"VOIE\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FANTOIR1016_974_WAYS[\"WAY\"] = FANTOIR1016_974_WAYS[[\"LIBELLE\", \"VOIE\"]].apply(lambda x: ' '.join(x), axis=1)\n",
    "FANTOIR1016_974_WAYS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a clean and official list of street name for our region, let identify which street name are not in listed in this reference (street name will be uppercase and with not latin letter for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtd = {ord(u'’'): u\"'\", ord(u'é'): u'e', ord(u'è'): u'e', ord(u'É'): u'E',}\n",
    "def tr(x):\n",
    "    return x.translate(xtd).upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addresses.loc[:, (\"street\")] = addresses.street.dropna().apply(lambda x: tr(x))\n",
    "addresses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addresses[\"CHECKED\"] = addresses.street.dropna().apply(lambda x: x in FANTOIR1016_974_WAYS.WAY.values)\n",
    "addresses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addresses[addresses.CHECKED == False].street.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addresses[addresses.CHECKED == False].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what could be the problems..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FANTOIR1016_974_WAYS[FANTOIR1016_974_WAYS.VOIE.str.find(\"MARTINEL\") != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two possibilities... cannot be solved at this level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FANTOIR1016_974_WAYS[FANTOIR1016_974_WAYS.VOIE.str.find(\"CHARRIE\") != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bad spelling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FANTOIR1016_974_WAYS[FANTOIR1016_974_WAYS.WAY.str.find(\"AVENUE DU GEN\") != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have some shortcut... to simplify the update, all names not referenced will be manually updated from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "streets = pd.DataFrame(addresses[addresses.CHECKED == False].street.unique())\n",
    "streets[\"New\"] = streets[0]\n",
    "streets.columns = [\"Old\", \"New\"]\n",
    "streets.to_csv(\"street_name_update.csv\", \n",
    "               encoding='utf-8',\n",
    "               index=False,\n",
    "               quoting=2\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read back the updated file (not completly updated... to show case only!!)\n",
    "streets = pd.read_csv(\"street_name_update.csv\")\n",
    "streets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "streetNames = dict(zip(streets.Old, streets.New))\n",
    "def streetNameUpdate(name):\n",
    "    return streetNames[name] if name in streetNames.keys() else name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addresses.street.apply(lambda x: streetNameUpdate(x)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have identify some rules to (partially) update the addresses, why not updating the mongo database..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Postal Code and City **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure we have no space in postcode (should have enforced as integer...)\n",
    "for n in db.LaReunion.find({ \"type\": \"node\", \n",
    "                            \"address\": { \"$exists\": True }, \n",
    "                            \"address.postcode\": { \"$exists\": True } }):\n",
    "    postcode = n[\"address\"][\"postcode\"].replace(' ', '')\n",
    "    db.LaReunion.update_one({ \"_id\": n[\"_id\"] }, { \"$set\": { \"address.postcode\":  postcode } }, upsert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n in db.LaReunion.find({ \"type\": \"node\", \n",
    "                            \"address\": { \"$exists\": True }, \n",
    "                            \"address.postcode\": { \"$exists\": True } }):\n",
    "    city = codes[int(n[\"address\"][\"postcode\"])]\n",
    "    db.LaReunion.update_one({ \"_id\": n[\"_id\"] }, { \"$set\": { \"address.city\":  city } }, upsert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    { \"$match\": { \"type\": \"node\", \"address\": { \"$exists\": True }, \"address.postcode\": { \"$exists\": True } } },\n",
    "    { \"$project\": { \"_id\": False, \n",
    "                   \"city\": \"$address.city\", \n",
    "                   \"housenumber\": \"$address.housenumber\", \n",
    "                   \"postcode\": \"$address.postcode\", \n",
    "                   \"street\": \"$address.street\" } },\n",
    "    { \"$limit\": 10 }\n",
    "]\n",
    "pd.DataFrame(list(db.LaReunion.aggregate(pipeline)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n in db.LaReunion.find({ \n",
    "        \"type\": \"node\", \n",
    "        \"address\": { \"$exists\": True }, \n",
    "        \"address.city\": { \"$exists\": True },\n",
    "        \"address.postcode\": { \"$exists\": False }}):\n",
    "    postcode = reviewCityNameForLookup(cleanName(n[\"address\"][\"city\"]))\n",
    "    db.LaReunion.update_one({ \"_id\": n[\"_id\"] }, { \"$set\": { \"address.postcode\":  postcode } }, upsert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pretty(list(db.LaReunion.find({ \n",
    "        \"type\": \"node\", \n",
    "        \"address\": { \"$exists\": True }, \n",
    "        \"address.city\": { \"$exists\": False },\n",
    "        \"address.postcode\": { \"$exists\": False }}).limit(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still some cleaning to be done... but not possible with the information we have in hands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Conclusion\n",
    "\n",
    "The dataset is far not complete and accurate. The different steps followed during the cleansing process has demonstrated that the dataset can be completed and made more accurate with some simple rules and correlation with some external databases. The process did not delivered a clean and accurate database as a lot a of specific rules need to be put in place as well as a better identification of external database that will make possible an automated process.\n",
    "\n",
    "I think it will be interesting to have an automated validation of the data during the update of OpenStreetMap, such as crosscheck of city, postcode, street name from external and official database on per country basis (if it does exists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
